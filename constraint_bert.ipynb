{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "constraint_bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "teS3JOeQYvWD",
        "outputId": "58241a1b-14d9-4848-fd31-9def6e406b61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgu0c-mqZOSy"
      },
      "source": [
        "class BERT(object):\n",
        "    def __init__(self, x, y, distilBERT=True, fullBERT=False):\n",
        "        super(BERT, self).__init__()\n",
        "        self.labels = y\n",
        "        \n",
        "        # make it list\n",
        "        self.data_x = x\n",
        "        self.features = None\n",
        "        self.padded = None\n",
        "        self.attention_m = None\n",
        "\n",
        "        self.distilBERT = distilBERT\n",
        "        self.fullBERT = fullBERT\n",
        "\n",
        "        if distilBERT:\n",
        "            # For DistilBERT:\n",
        "            self.model_class, self.tokenizer_class, self.pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "        elif fullBERT:\n",
        "            # Want BERT instead of distilBERT? \n",
        "            self.model_class, self.tokenizer_class, self.pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "        # Load pretrained model/tokenizer\n",
        "        self.tokenizer = self.tokenizer_class.from_pretrained(self.pretrained_weights)\n",
        "        self.model = self.model_class.from_pretrained(self.pretrained_weights)\n",
        "    \n",
        "    # tokeise sentences into index numbers\n",
        "    def bert_tokenise(self):\n",
        "        tokenized = self.data_x.apply((lambda x: self.tokenizer.encode(x, add_special_tokens=True)))\n",
        "        return tokenized\n",
        "\n",
        "    # pad the tokens with\n",
        "    def padding(self, list_of_tokenized):\n",
        "        print(\"starting to perform padding on the input length of: \", len(list_of_tokenized))\n",
        "        max_len = 0\n",
        "        for i in list_of_tokenized.values:\n",
        "            if len(i) > max_len:\n",
        "                max_len = len(i)\n",
        "        self.padded = np.array([i + [0]*(max_len-len(i)) for i in list_of_tokenized.values])\n",
        "        print(\"Padding over and we have padded array of shape: \", (self.padded).shape)\n",
        "        return self\n",
        "\n",
        "    def attention_mask(self):\n",
        "        self.attention_m = np.where(self.padded != 0, 1, 0)\n",
        "        print(\"Done with attention mask. Shape of attention mask matrix is: \", self.attention_m.shape)\n",
        "        return self\n",
        "    \n",
        "    def bert_train(self):\n",
        "        input_ids = torch.tensor(self.padded)\n",
        "        attention_mask = torch.tensor(self.attention_m)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            last_hidden_states = self.model(input_ids, attention_mask=attention_mask)\n",
        "        self.features = last_hidden_states[0][:,0,:].numpy()\n",
        "        self.last_hidden_states = last_hidden_states\n",
        "        return self\n",
        "\n",
        "    # def split(self):\n",
        "    #     train_features, test_features, train_labels, test_labels = train_test_split(self.features, self.labels)\n",
        "    #     return train_features, test_features, train_labels, test_labels\n",
        "    \n",
        "    def sigmoid_model(self, params=[0.0001, 100, 200]):\n",
        "        #split data\n",
        "        train_features, test_features, train_labels, test_labels = self.features[:4494], self.features[4494:], self.labels[:4494], self.labels[4494:]\n",
        "        \n",
        "        # parameter_tuning\n",
        "        parameters = {'C': params}\n",
        "        grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
        "        grid_search.fit(train_features, train_labels)\n",
        "\n",
        "        print('best parameters: ', grid_search.best_params_)\n",
        "        print('best scrores: ', grid_search.best_score_)\n",
        "\n",
        "        # sigmoid_model\n",
        "        lr_clf = LogisticRegression(C=grid_search.best_params_['C'])\n",
        "        lr_clf.fit(train_features, train_labels)\n",
        "        acc_test = lr_clf.score(test_features, test_labels)\n",
        "        print(\"Accuracy Score on Test Data: \", acc_test*100)\n",
        "        y_pred = lr_clf.predict(test_features)\n",
        "        prf = precision_recall_fscore_support(test_labels, y_pred, average='weighted')\n",
        "        print(\"Precision Score on Test Data: \", prf[0])\n",
        "        print(\"Recall Score on Test Data: \", prf[1])\n",
        "        print(\"F1-Score Score on Test Data: \", prf[2])\n",
        "        \n",
        "\n",
        "        "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBdfziSvgnmn",
        "outputId": "2de66acd-8c42-4eed-d444-68eb6dd8ac95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# comment this later and use your own data\n",
        "# df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n",
        "df1 = pd.read_csv('processed_tweets.csv')\n",
        "df2 = pd.read_csv('processed_tweets_val.csv')\n",
        "df = pd.concat([df1, df2])\n",
        "\n",
        "# batch_1 = df[:200]\n",
        "batch_1 = df\n",
        "\n",
        "def sentence_truncate(x):\n",
        "        return x[:min(len(x), 511)]\n",
        "    \n",
        "batch_1['tweets'] = batch_1['tweets'].map(sentence_truncate)\n",
        "\n",
        "# uncomment this with our data\n",
        "x, y = batch_1['tweets'], batch_1['label']\n",
        "\n",
        "# x,y = batch_1[0], batch_1[1]\n",
        "\n",
        "bert = BERT(x,y)\n",
        "tokenized = bert.bert_tokenise()\n",
        "bert.padding(tokenized)\n",
        "bert.attention_mask()\n",
        "res = bert.bert_train()\n",
        "bert.sigmoid_model(params=[0.1,0.001,0.0001])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting to perform padding on the input length of:  200\n",
            "Padding over and we have padded array of shape:  (200, 83)\n",
            "Done with attention mask. Shape of attention mask matrix is:  (200, 83)\n",
            "best parameters:  {'C': 0.1}\n",
            "best scrores:  0.8935672514619881\n",
            "Accuracy Score on Test Data:  88.67924528301887\n",
            "Precision Score on Test Data:  0.8955347756321586\n",
            "Recall Score on Test Data:  0.8867924528301887\n",
            "F1-Score Score on Test Data:  0.8859791802212102\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}